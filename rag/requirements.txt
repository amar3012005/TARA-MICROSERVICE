# ============================================================================
# OPTIMIZED ORDER FOR DOCKER LAYER CACHING
# Heavy packages installed FIRST so they're cached when lighter packages change
# ============================================================================

# HEAVIEST: Core ML frameworks (install FIRST for best caching)
# These rarely change and take longest to download/install
torch>=2.0.0
numpy==1.24.3
faiss-cpu>=1.7.4

# HEAVY: ML/Embeddings frameworks (depend on torch/numpy)
# These are large but more stable than application code
sentence-transformers>=2.2.0
huggingface-hub>=0.16.0
langchain-huggingface>=0.0.1

# MEDIUM: API clients and frameworks
google-genai>=1.33.0
google-generativeai>=0.8.0

# LIGHT: Application framework (changes more frequently)
fastapi>=0.115.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0

# LIGHTEST: Utilities and helpers (change most frequently)
python-multipart>=0.0.6
redis[asyncio]>=5.0.0
httpx>=0.24.0
python-dotenv==1.0.1

