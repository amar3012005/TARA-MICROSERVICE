services:
  redis:
    image: redis:7-alpine
    container_name: leibniz-redis
    ports:
      - "6381:6381"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --port 6381 --bind 0.0.0.0
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6381", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    # networks:
    #   - leibniz-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  stt-vad-service:
    build:
      context: .
      dockerfile: stt_vad/Dockerfile
    container_name: stt-vad-service
    ports:
      - "8001:8001"
      - "7860:7860"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - PYTHONUNBUFFERED=1
      - STT_VAD_HOST=0.0.0.0
      - STT_VAD_PORT=8001
      - FAST_RTC_PORT=7860
      - REDIS_HOST=host.docker.internal
      - REDIS_PORT=6381
      - LEIBNIZ_REDIS_HOST=host.docker.internal
      - LEIBNIZ_REDIS_PORT=6381
      - GEMINI_API_KEY=${GEMINI_API_KEY:-AIzaSyAPNe7vAaTeRtjCU13JEf-wbbabPhsk5Gw}
      - DOCKER_ENVIRONMENT=true
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - leibniz-network
    restart: unless-stopped
    stdin_open: true
    tty: true
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  intent-service:
    build:
      context: ..
      dockerfile: services/intent/Dockerfile
    container_name: intent-service
    # ports:
    #   - "8010:8002"
    network_mode: "host"
    environment:
      - PYTHONUNBUFFERED=1
      - GEMINI_API_KEY=${GEMINI_API_KEY:-AIzaSyAPNe7vAaTeRtjCU13JEf-wbbabPhsk5Gw}
      - GEMINI_MODEL=gemini-2.5-flash-lite
      - REDIS_URL=redis://localhost:6381
      - INTENT_CACHE_TTL=1800
    depends_on:
      redis:
        condition: service_healthy
    # networks:
    #   - leibniz-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  rag-service:
    image: leibniz-rag:latest  # Use pre-built image
    build:
      context: ..
      dockerfile: services/rag/Dockerfile
    container_name: rag-service
    # ports:
    #   - "8003:8003"
    network_mode: "host"
    environment:
      - PYTHONUNBUFFERED=1
      - GEMINI_API_KEY=${GEMINI_API_KEY:-AIzaSyAPNe7vAaTeRtjCU13JEf-wbbabPhsk5Gw}
      - GEMINI_MODEL=gemini-2.0-flash-lite
      - LEIBNIZ_REDIS_HOST=localhost
      - LEIBNIZ_REDIS_PORT=6381
      - LEIBNIZ_REDIS_DB=0
      - LEIBNIZ_RAG_KNOWLEDGE_BASE_PATH=/app/leibniz_knowledge_base
      - LEIBNIZ_RAG_VECTOR_STORE_PATH=/app/index
      - LEIBNIZ_RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - LEIBNIZ_RAG_TOP_K=8
      - LEIBNIZ_RAG_TOP_N=5
      - LEIBNIZ_RAG_SIMILARITY_THRESHOLD=0.3
      - LEIBNIZ_RAG_CACHE_TTL=3600
      - LEIBNIZ_RAG_ENABLE_HYBRID_SEARCH=true
      - PORT=8003
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    # networks:
    #   - leibniz-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  orchestrator:
    build:
      context: .
      dockerfile: orchestrator/Dockerfile
    container_name: orchestrator-service
    ports:
      - "8004:8004"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://host.docker.internal:6381/0
      - LEIBNIZ_REDIS_HOST=host.docker.internal
      - LEIBNIZ_REDIS_PORT=6381
      - LEIBNIZ_REDIS_DB=0
      - INTENT_SERVICE_URL=http://host.docker.internal:8002
      - RAG_SERVICE_URL=http://host.docker.internal:8003
      - STT_SERVICE_URL=http://host.docker.internal:8001
      - TTS_SERVICE_URL=http://host.docker.internal:8005
      - INTRO_GREETING=${INTRO_GREETING:-Hello! I'm your assistant at Leibniz University. How can I help you today?}
      - LOG_LEVEL=INFO
      - SESSION_TTL_SECONDS=3600
      - MAX_CONCURRENT_SESSIONS=1000
    depends_on:
      redis:
        condition: service_healthy
      intent-service:
        condition: service_started
      rag-service:
        condition: service_started
      stt-vad-service:
        condition: service_started
      tts-streaming-service-new:
        condition: service_started
    # networks:
    #   - leibniz-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  tts-streaming-service-new:
    build:
      context: .
      dockerfile: tts_streaming/Dockerfile
    container_name: tts-streaming-service-v3
    ports:
      - "8005:8005"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - PYTHONUNBUFFERED=1
      - LEMONFOX_API_KEY=${LEMONFOX_API_KEY:-6KtIDvYHZAlfKcBpjwqlb1g1XdUksVM2}
      - LEIBNIZ_LEMONFOX_VOICE=${LEIBNIZ_LEMONFOX_VOICE:-sarah}
      - LEIBNIZ_LEMONFOX_LANGUAGE=${LEIBNIZ_LEMONFOX_LANGUAGE:-en-us}
      - TTS_STREAMING_PORT=8005
      - LEIBNIZ_TTS_CACHE_DIR=/app/audio_cache
      - LEIBNIZ_TTS_CACHE_ENABLED=true
      - LEIBNIZ_TTS_CACHE_MAX_SIZE=500
      - LEIBNIZ_TTS_SAMPLE_RATE=24000
      - TTS_QUEUE_MAX_SIZE=10
      - REDIS_HOST=host.docker.internal
      - REDIS_PORT=6381
      - LEIBNIZ_REDIS_HOST=host.docker.internal
      - LEIBNIZ_REDIS_PORT=6381
    networks:
      - leibniz-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  stt-local-service:
    build:
      context: .
      dockerfile: stt_local/Dockerfile
    container_name: stt-local-service
    # ports:
    #   - "8014:8006"
    #   - "7863:7861"
    network_mode: "host"
    environment:
      - PYTHONUNBUFFERED=1
      - STT_LOCAL_HOST=0.0.0.0
      - STT_LOCAL_PORT=8006
      - FAST_RTC_PORT=7861
      - REDIS_HOST=localhost
      - REDIS_PORT=6381
      - LEIBNIZ_REDIS_HOST=localhost
      - LEIBNIZ_REDIS_PORT=6381
      - LEIBNIZ_REDIS_DB=0
      - LEIBNIZ_STT_LOCAL_WHISPER_MODEL_SIZE=base
      - LEIBNIZ_STT_LOCAL_WHISPER_DEVICE=cuda
      - LEIBNIZ_STT_LOCAL_WHISPER_COMPUTE_TYPE=float16
      - LEIBNIZ_STT_LOCAL_WHISPER_LANGUAGE=en
      - LEIBNIZ_STT_LOCAL_USE_GPU=true
      - LEIBNIZ_STT_LOCAL_VAD_THRESHOLD=0.5
      - LEIBNIZ_STT_LOCAL_VAD_MIN_SPEECH_MS=250
      - LEIBNIZ_STT_LOCAL_VAD_SILENCE_TIMEOUT_MS=800
      - LEIBNIZ_STT_LOCAL_PARTIAL_UPDATE_INTERVAL_MS=500
      - DOCKER_ENVIRONMENT=true
    depends_on:
      redis:
        condition: service_healthy
    # networks:
    #   - leibniz-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    stdin_open: true
    tty: true
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  redis_data:

networks:
  leibniz-network:
    name: services_leibniz-network
    driver: bridge
    external: false
