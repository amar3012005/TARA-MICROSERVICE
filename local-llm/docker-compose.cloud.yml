services:
  rag-llm-service:
    image: vllm/vllm-openai:v0.12.0
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      VLLM_MAX_NUM_TOKENS: 4096
      VLLM_MAX_MODEL_LEN: 8192
      VLLM_MAX_BATCH_SIZE: 32
      VLLM_MAX_NUM_SEQS: 32
      VLLM_GPU_MEMORY_UTILIZATION: 0.9
      VLLM_LOGGING_LEVEL: info
      OTLP_TRACE_ENDPOINT: ${OTLP_TRACE_ENDPOINT:-}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${MODEL_CACHE_DIR:-/var/lib/llm-cache}:/root/.cache/huggingface:rw
    ports:
      - "8081:8081"
    command: >-
      --model ${LLM_MODEL_ID:-Qwen/Qwen2.5-7B-Instruct-AWQ}
      --quantization awq
      --kv-cache-dtype fp8
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --max-num-seqs 32
      --gpu-memory-utilization 0.9
      --enable-chunked-prefill
      --port 8081
    healthcheck:
      test: ["CMD", "curl", "-s", "http://localhost:8081/health"]
      interval: 20s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped
